{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-vUMUpfueJAb",
        "outputId": "720efb1a-60a7-4dda-9d4b-4d5027d7adbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== CROSS-SOURCE RECORD LINKING: SYNTHETIC TRAINING DATASET ===\n",
            "Building intelligent training data from your invoice datasets...\n",
            "\n",
            "‚úÖ Source A loaded: 320 records, 9 columns\n",
            "‚úÖ Source B loaded: 345 records, 9 columns\n",
            "\n",
            "üìä Source A columns: ['invoice_id', 'po_number', 'customer_name', 'customer_email', 'amount', 'tax_amount', 'total_amount', 'currency', 'invoice_date']\n",
            "üìä Source B columns: ['ref_code', 'purchase_order', 'client', 'email', 'net', 'tax', 'grand_total', 'ccy', 'doc_date']\n",
            "\n",
            "üîç Sample from Source A:\n",
            "       invoice_id customer_name        customer_email  total_amount  \\\n",
            "0  INV-2025688815    Riya Singh  riyasingh@sample.net     173423.84   \n",
            "1  INV-2025639985    Vihaan Rao     vihaanrao@demo.co      25016.15   \n",
            "2  INV-2025297638     Riya Nair  riyanair@example.com     181772.75   \n",
            "\n",
            "  invoice_date  \n",
            "0   2025-06-07  \n",
            "1   2025-08-05  \n",
            "2   2025-08-05  \n",
            "\n",
            "üîç Sample from Source B:\n",
            "         ref_code      client                 email  grand_total    doc_date\n",
            "0  INV-2025688815  Riya Singh  riyasingh@sample.net    173423.84  2025-06-06\n",
            "1     REF-985-189  Vihaan Rao     vihaanrao@demo.co     25014.15  2025-08-05\n",
            "2     REF-638-401   Riya Nair  riyanair@example.com    181772.75  2025-08-05\n",
            "\n",
            "‚úÖ Ready to create synthetic training dataset!\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Foundation Setup and Data Loading\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import random\n",
        "from typing import Dict, List, Tuple, Any\n",
        "from datetime import datetime, timedelta\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"=== CROSS-SOURCE RECORD LINKING: SYNTHETIC TRAINING DATASET ===\")\n",
        "print(\"Building intelligent training data from your invoice datasets...\\n\")\n",
        "\n",
        "# Load the actual data files\n",
        "source_a = pd.read_csv('Project7SourceA.csv')\n",
        "source_b = pd.read_csv('Project7SourceB.csv')\n",
        "\n",
        "print(f\"‚úÖ Source A loaded: {source_a.shape[0]} records, {source_a.shape[1]} columns\")\n",
        "print(f\"‚úÖ Source B loaded: {source_b.shape[0]} records, {source_b.shape[1]} columns\")\n",
        "\n",
        "# Display column structure\n",
        "print(f\"\\nüìä Source A columns: {list(source_a.columns)}\")\n",
        "print(f\"üìä Source B columns: {list(source_b.columns)}\")\n",
        "\n",
        "# Quick data preview\n",
        "print(f\"\\nüîç Sample from Source A:\")\n",
        "print(source_a[['invoice_id', 'customer_name', 'customer_email', 'total_amount', 'invoice_date']].head(3))\n",
        "\n",
        "print(f\"\\nüîç Sample from Source B:\")\n",
        "print(source_b[['ref_code', 'client', 'email', 'grand_total', 'doc_date']].head(3))\n",
        "\n",
        "print(f\"\\n‚úÖ Ready to create synthetic training dataset!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Build the Synthetic Training Dataset Generator\n",
        "class SyntheticTrainingGenerator:\n",
        "    \"\"\"\n",
        "    Generates high-quality training data for record linking models\n",
        "    by creating controlled transformations and negative examples\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, source_a: pd.DataFrame, source_b: pd.DataFrame):\n",
        "        self.source_a = source_a.copy()\n",
        "        self.source_b = source_b.copy()\n",
        "        self.transformation_patterns = []\n",
        "        self.field_mappings = {\n",
        "            'id': ('invoice_id', 'ref_code'),\n",
        "            'name': ('customer_name', 'client'),\n",
        "            'email': ('customer_email', 'email'),\n",
        "            'amount': ('total_amount', 'grand_total'),\n",
        "            'date': ('invoice_date', 'doc_date'),\n",
        "            'po': ('po_number', 'purchase_order')\n",
        "        }\n",
        "\n",
        "    def generate_id_transformations(self, base_id: str) -> List[str]:\n",
        "        \"\"\"Generate various ID transformation patterns\"\"\"\n",
        "        transformations = []\n",
        "\n",
        "        # Extract numeric part if exists\n",
        "        numeric_match = re.search(r'\\d+', str(base_id))\n",
        "        if not numeric_match:\n",
        "            return [base_id]\n",
        "\n",
        "        base_number = numeric_match.group()\n",
        "\n",
        "        # Pattern 1: INV-123456 ‚Üí 2025123456 (remove prefix, add year)\n",
        "        transformations.append(f\"2025{base_number}\")\n",
        "\n",
        "        # Pattern 2: INV-123456 ‚Üí REF-123-456 (change prefix, add dashes)\n",
        "        if len(base_number) >= 6:\n",
        "            mid_point = len(base_number) // 2\n",
        "            transformations.append(f\"REF-{base_number[:mid_point]}-{base_number[mid_point:]}\")\n",
        "\n",
        "        # Pattern 3: INV-123456 ‚Üí #2025::123456 (hash format)\n",
        "        transformations.append(f\"#2025::{base_number}\")\n",
        "\n",
        "        # Pattern 4: INV-123456 ‚Üí 2025/123456 (slash format)\n",
        "        transformations.append(f\"2025/{base_number}\")\n",
        "\n",
        "        # Pattern 5: INV-123456 ‚Üí 2025-123456 (dash format)\n",
        "        transformations.append(f\"2025-{base_number}\")\n",
        "\n",
        "        return transformations\n",
        "\n",
        "    def add_noise_to_field(self, value: str, field_type: str, noise_level: float = 0.1) -> str:\n",
        "        \"\"\"Add realistic noise to field values\"\"\"\n",
        "        if pd.isna(value) or value == '':\n",
        "            return value\n",
        "\n",
        "        value = str(value)\n",
        "\n",
        "        if field_type == 'name':\n",
        "            # Name variations: abbreviations, typos\n",
        "            if random.random() < noise_level:\n",
        "                if ' ' in value:\n",
        "                    parts = value.split()\n",
        "                    # Sometimes use just first name\n",
        "                    if random.random() < 0.5:\n",
        "                        return parts[0]\n",
        "                return value\n",
        "\n",
        "        elif field_type == 'email':\n",
        "            # Email variations: domain changes, @ symbol issues\n",
        "            if random.random() < noise_level:\n",
        "                if '@' in value:\n",
        "                    local, domain = value.split('@', 1)\n",
        "                    # Sometimes break the @ symbol or change domain\n",
        "                    if random.random() < 0.3:\n",
        "                        return f\"{local} @ {domain}\"\n",
        "                    elif random.random() < 0.3:\n",
        "                        # Change domain extension\n",
        "                        domain_base = domain.split('.')[0]\n",
        "                        new_domains = ['demo.co', 'test.org', 'example.com', 'sample.net']\n",
        "                        return f\"{local}@{domain_base}.{random.choice(['co', 'org', 'com', 'net'])}\"\n",
        "\n",
        "        elif field_type == 'amount':\n",
        "            # Amount variations: small rounding differences\n",
        "            if random.random() < noise_level:\n",
        "                try:\n",
        "                    amount = float(value)\n",
        "                    # Add small random variation (¬±0.5%)\n",
        "                    variation = amount * random.uniform(-0.005, 0.005)\n",
        "                    return str(round(amount + variation, 2))\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "        elif field_type == 'date':\n",
        "            # Date variations: ¬±1 day drift\n",
        "            if random.random() < noise_level:\n",
        "                try:\n",
        "                    date_obj = pd.to_datetime(value)\n",
        "                    drift_days = random.choice([-1, 0, 1])\n",
        "                    new_date = date_obj + timedelta(days=drift_days)\n",
        "                    return new_date.strftime('%Y-%m-%d')\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "        return value\n",
        "\n",
        "# Initialize the generator\n",
        "generator = SyntheticTrainingGenerator(source_a, source_b)\n",
        "print(\"‚úÖ Synthetic Training Generator initialized\")\n",
        "print(\"‚úÖ Ready to generate training pairs\")\n",
        "\n",
        "# Test ID transformations\n",
        "sample_id = source_a.iloc[0]['invoice_id']\n",
        "transformations = generator.generate_id_transformations(sample_id)\n",
        "print(f\"\\nüîç Sample ID transformations for '{sample_id}':\")\n",
        "for i, transform in enumerate(transformations):\n",
        "    print(f\"  {i+1}: {transform}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3RVIPp0Lg6j0",
        "outputId": "9a10637b-37f3-4047-d53a-acf9fa84a2ac"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Synthetic Training Generator initialized\n",
            "‚úÖ Ready to generate training pairs\n",
            "\n",
            "üîç Sample ID transformations for 'INV-2025688815':\n",
            "  1: 20252025688815\n",
            "  2: REF-20256-88815\n",
            "  3: #2025::2025688815\n",
            "  4: 2025/2025688815\n",
            "  5: 2025-2025688815\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Add methods to create positive and negative training pairs\n",
        "def create_positive_pairs(self, num_pairs: int = 200) -> List[Dict]:\n",
        "    \"\"\"Create positive training examples with controlled transformations\"\"\"\n",
        "    positive_pairs = []\n",
        "\n",
        "    for i in range(min(num_pairs, len(self.source_a) * 3)):  # Allow multiple transformations per record\n",
        "        # Take a record from source A (cycle through records)\n",
        "        record_a = self.source_a.iloc[i % len(self.source_a)]\n",
        "\n",
        "        # Create transformed version for source B\n",
        "        transformed_record = {}\n",
        "\n",
        "        # Transform ID with pattern\n",
        "        base_id = record_a['invoice_id']\n",
        "        id_transformations = self.generate_id_transformations(base_id)\n",
        "        transformed_record['ref_code'] = random.choice(id_transformations)\n",
        "\n",
        "        # Transform other fields with noise\n",
        "        transformed_record['client'] = self.add_noise_to_field(\n",
        "            record_a['customer_name'], 'name', noise_level=0.2\n",
        "        )\n",
        "        transformed_record['email'] = self.add_noise_to_field(\n",
        "            record_a['customer_email'], 'email', noise_level=0.15\n",
        "        )\n",
        "        transformed_record['grand_total'] = self.add_noise_to_field(\n",
        "            record_a['total_amount'], 'amount', noise_level=0.1\n",
        "        )\n",
        "        transformed_record['doc_date'] = self.add_noise_to_field(\n",
        "            record_a['invoice_date'], 'date', noise_level=0.1\n",
        "        )\n",
        "        transformed_record['purchase_order'] = record_a['po_number']\n",
        "\n",
        "        # Create training pair\n",
        "        pair = {\n",
        "            'source_a_idx': i % len(self.source_a),\n",
        "            'source_b_idx': -1,  # Virtual record\n",
        "            'label': 1,  # Positive match\n",
        "            'confidence': random.uniform(0.85, 1.0),\n",
        "            'record_a': record_a.to_dict(),\n",
        "            'record_b': transformed_record,\n",
        "            'transformation_type': 'synthetic_positive'\n",
        "        }\n",
        "\n",
        "        positive_pairs.append(pair)\n",
        "\n",
        "    return positive_pairs\n",
        "\n",
        "def create_negative_pairs(self, num_pairs: int = 300) -> List[Dict]:\n",
        "    \"\"\"Create negative training examples from unrelated records\"\"\"\n",
        "    negative_pairs = []\n",
        "\n",
        "    attempts = 0\n",
        "    while len(negative_pairs) < num_pairs and attempts < num_pairs * 3:\n",
        "        attempts += 1\n",
        "\n",
        "        # Random record from source A\n",
        "        idx_a = random.randint(0, len(self.source_a) - 1)\n",
        "        record_a = self.source_a.iloc[idx_a]\n",
        "\n",
        "        # Random record from source B\n",
        "        idx_b = random.randint(0, len(self.source_b) - 1)\n",
        "        record_b = self.source_b.iloc[idx_b]\n",
        "\n",
        "        # Ensure they're different records (avoid accidental matches)\n",
        "        if (record_a['customer_name'] == record_b['client'] and\n",
        "            abs(float(record_a['total_amount']) - float(record_b['grand_total'])) < 10):\n",
        "            continue  # Skip this pair, too similar\n",
        "\n",
        "        pair = {\n",
        "            'source_a_idx': idx_a,\n",
        "            'source_b_idx': idx_b,\n",
        "            'label': 0,  # Negative match\n",
        "            'confidence': random.uniform(0.0, 0.3),\n",
        "            'record_a': record_a.to_dict(),\n",
        "            'record_b': record_b.to_dict(),\n",
        "            'transformation_type': 'negative'\n",
        "        }\n",
        "\n",
        "        negative_pairs.append(pair)\n",
        "\n",
        "    return negative_pairs\n",
        "\n",
        "def generate_training_dataset(self, positive_pairs: int = 500, negative_pairs: int = 800):\n",
        "    \"\"\"Generate complete training dataset\"\"\"\n",
        "    print(\"üèóÔ∏è  Generating synthetic training dataset...\")\n",
        "\n",
        "    # Create positive and negative pairs\n",
        "    pos_pairs = self.create_positive_pairs(positive_pairs)\n",
        "    neg_pairs = self.create_negative_pairs(negative_pairs)\n",
        "\n",
        "    all_pairs = pos_pairs + neg_pairs\n",
        "    random.shuffle(all_pairs)  # Shuffle for better training\n",
        "\n",
        "    print(f\"‚úÖ Generated {len(pos_pairs)} positive pairs\")\n",
        "    print(f\"‚úÖ Generated {len(neg_pairs)} negative pairs\")\n",
        "    print(f\"‚úÖ Total training examples: {len(all_pairs)}\")\n",
        "\n",
        "    return all_pairs\n",
        "\n",
        "# Add methods to the SyntheticTrainingGenerator class\n",
        "SyntheticTrainingGenerator.create_positive_pairs = create_positive_pairs\n",
        "SyntheticTrainingGenerator.create_negative_pairs = create_negative_pairs\n",
        "SyntheticTrainingGenerator.generate_training_dataset = generate_training_dataset\n",
        "\n",
        "# Generate training dataset\n",
        "print(\"üéØ Generating large-scale training dataset...\")\n",
        "training_dataset = generator.generate_training_dataset(positive_pairs=500, negative_pairs=800)\n",
        "\n",
        "# Show sample results\n",
        "print(f\"\\nüìã Sample positive pair:\")\n",
        "pos_sample = [pair for pair in training_dataset if pair['label'] == 1][0]\n",
        "print(f\"Source A ID: {pos_sample['record_a']['invoice_id']}\")\n",
        "print(f\"Source B ID: {pos_sample['record_b']['ref_code']}\")\n",
        "print(f\"Source A Name: {pos_sample['record_a']['customer_name']}\")\n",
        "print(f\"Source B Name: {pos_sample['record_b']['client']}\")\n",
        "print(f\"Label: {pos_sample['label']} (Positive Match)\")\n",
        "\n",
        "print(f\"\\nüìã Sample negative pair:\")\n",
        "neg_sample = [pair for pair in training_dataset if pair['label'] == 0][0]\n",
        "print(f\"Source A Name: {neg_sample['record_a']['customer_name']}\")\n",
        "print(f\"Source B Name: {neg_sample['record_b']['client']}\")\n",
        "print(f\"Source A Amount: {neg_sample['record_a']['total_amount']}\")\n",
        "print(f\"Source B Amount: {neg_sample['record_b']['grand_total']}\")\n",
        "print(f\"Label: {neg_sample['label']} (Negative Match)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mZqfYzZHg-gn",
        "outputId": "37e06bd0-88d6-4858-9039-507d0997b757"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üéØ Generating large-scale training dataset...\n",
            "üèóÔ∏è  Generating synthetic training dataset...\n",
            "‚úÖ Generated 500 positive pairs\n",
            "‚úÖ Generated 800 negative pairs\n",
            "‚úÖ Total training examples: 1300\n",
            "\n",
            "üìã Sample positive pair:\n",
            "Source A ID: INV-2025545307\n",
            "Source B ID: 20252025545307\n",
            "Source A Name: Navya Bansal\n",
            "Source B Name: Navya Bansal\n",
            "Label: 1 (Positive Match)\n",
            "\n",
            "üìã Sample negative pair:\n",
            "Source A Name: Arjun Menon\n",
            "Source B Name: Arjun\n",
            "Source A Amount: 97014.64\n",
            "Source B Amount: 45678.18\n",
            "Label: 0 (Negative Match)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Feature Engineering Pipeline for Record Linking\n",
        "from difflib import SequenceMatcher\n",
        "\n",
        "class RecordLinkingFeatureExtractor:\n",
        "    \"\"\"\n",
        "    Extracts sophisticated features from record pairs for ML training\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.feature_names = []\n",
        "\n",
        "    def levenshtein_distance(self, s1: str, s2: str) -> int:\n",
        "        \"\"\"Calculate Levenshtein distance using dynamic programming\"\"\"\n",
        "        if len(s1) < len(s2):\n",
        "            return self.levenshtein_distance(s2, s1)\n",
        "\n",
        "        if len(s2) == 0:\n",
        "            return len(s1)\n",
        "\n",
        "        previous_row = list(range(len(s2) + 1))\n",
        "        for i, c1 in enumerate(s1):\n",
        "            current_row = [i + 1]\n",
        "            for j, c2 in enumerate(s2):\n",
        "                insertions = previous_row[j + 1] + 1\n",
        "                deletions = current_row[j] + 1\n",
        "                substitutions = previous_row[j] + (c1 != c2)\n",
        "                current_row.append(min(insertions, deletions, substitutions))\n",
        "            previous_row = current_row\n",
        "\n",
        "        return previous_row[-1]\n",
        "\n",
        "    def jaro_similarity(self, s1: str, s2: str) -> float:\n",
        "        \"\"\"Calculate Jaro similarity\"\"\"\n",
        "        if not s1 and not s2:\n",
        "            return 1.0\n",
        "        if not s1 or not s2:\n",
        "            return 0.0\n",
        "\n",
        "        # Calculate the match window\n",
        "        match_window = max(len(s1), len(s2)) // 2 - 1\n",
        "        match_window = max(0, match_window)\n",
        "\n",
        "        s1_matches = [False] * len(s1)\n",
        "        s2_matches = [False] * len(s2)\n",
        "\n",
        "        matches = 0\n",
        "        transpositions = 0\n",
        "\n",
        "        # Find matches\n",
        "        for i in range(len(s1)):\n",
        "            start = max(0, i - match_window)\n",
        "            end = min(i + match_window + 1, len(s2))\n",
        "\n",
        "            for j in range(start, end):\n",
        "                if s2_matches[j] or s1[i] != s2[j]:\n",
        "                    continue\n",
        "                s1_matches[i] = True\n",
        "                s2_matches[j] = True\n",
        "                matches += 1\n",
        "                break\n",
        "\n",
        "        if matches == 0:\n",
        "            return 0.0\n",
        "\n",
        "        # Count transpositions\n",
        "        k = 0\n",
        "        for i in range(len(s1)):\n",
        "            if not s1_matches[i]:\n",
        "                continue\n",
        "            while not s2_matches[k]:\n",
        "                k += 1\n",
        "            if s1[i] != s2[k]:\n",
        "                transpositions += 1\n",
        "            k += 1\n",
        "\n",
        "        jaro = (matches / len(s1) + matches / len(s2) +\n",
        "                (matches - transpositions / 2) / matches) / 3\n",
        "\n",
        "        return jaro\n",
        "\n",
        "    def string_similarity(self, str1: str, str2: str) -> Dict[str, float]:\n",
        "        \"\"\"Calculate multiple string similarity metrics\"\"\"\n",
        "        if pd.isna(str1) or pd.isna(str2):\n",
        "            return {'exact_match': 0.0, 'levenshtein': 0.0, 'jaro': 0.0, 'sequence_match': 0.0}\n",
        "\n",
        "        str1, str2 = str(str1).lower().strip(), str(str2).lower().strip()\n",
        "\n",
        "        # Exact match\n",
        "        exact_match = 1.0 if str1 == str2 else 0.0\n",
        "\n",
        "        # Levenshtein distance (normalized)\n",
        "        max_len = max(len(str1), len(str2))\n",
        "        levenshtein = 1.0 - (self.levenshtein_distance(str1, str2) / max_len) if max_len > 0 else 0.0\n",
        "\n",
        "        # Jaro similarity\n",
        "        jaro = self.jaro_similarity(str1, str2)\n",
        "\n",
        "        # Sequence matcher\n",
        "        sequence_match = SequenceMatcher(None, str1, str2).ratio()\n",
        "\n",
        "        return {\n",
        "            'exact_match': exact_match,\n",
        "            'levenshtein': levenshtein,\n",
        "            'jaro': jaro,\n",
        "            'sequence_match': sequence_match\n",
        "        }\n",
        "\n",
        "    def extract_numeric_core(self, id_str: str) -> str:\n",
        "        \"\"\"Extract numeric core from ID string\"\"\"\n",
        "        if pd.isna(id_str):\n",
        "            return \"\"\n",
        "\n",
        "        # Extract all digits\n",
        "        digits = re.findall(r'\\d+', str(id_str))\n",
        "        return ''.join(digits) if digits else \"\"\n",
        "\n",
        "# Initialize feature extractor\n",
        "feature_extractor = RecordLinkingFeatureExtractor()\n",
        "print(\"‚úÖ Feature extractor initialized\")\n",
        "\n",
        "# Test string similarity on sample data\n",
        "sample_pair = training_dataset[0]\n",
        "name_similarity = feature_extractor.string_similarity(\n",
        "    sample_pair['record_a']['customer_name'],\n",
        "    sample_pair['record_b']['client']\n",
        ")\n",
        "\n",
        "print(f\"\\nüîç String similarity test:\")\n",
        "print(f\"Name A: {sample_pair['record_a']['customer_name']}\")\n",
        "print(f\"Name B: {sample_pair['record_b']['client']}\")\n",
        "print(f\"Similarities: {name_similarity}\")\n",
        "\n",
        "# Test ID core extraction\n",
        "id_a = sample_pair['record_a']['invoice_id']\n",
        "id_b = sample_pair['record_b']['ref_code']\n",
        "core_a = feature_extractor.extract_numeric_core(id_a)\n",
        "core_b = feature_extractor.extract_numeric_core(id_b)\n",
        "\n",
        "print(f\"\\nüîç ID core extraction test:\")\n",
        "print(f\"ID A: {id_a} ‚Üí Core: {core_a}\")\n",
        "print(f\"ID B: {id_b} ‚Üí Core: {core_b}\")\n",
        "print(f\"Cores match: {core_a == core_b}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uk2fQF_ChNuB",
        "outputId": "5e46195c-1b90-4d59-81a7-60d6fcaf15fc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Feature extractor initialized\n",
            "\n",
            "üîç String similarity test:\n",
            "Name A: Navya Bansal\n",
            "Name B: Navya Bansal\n",
            "Similarities: {'exact_match': 1.0, 'levenshtein': 1.0, 'jaro': 1.0, 'sequence_match': 1.0}\n",
            "\n",
            "üîç ID core extraction test:\n",
            "ID A: INV-2025545307 ‚Üí Core: 2025545307\n",
            "ID B: 20252025545307 ‚Üí Core: 20252025545307\n",
            "Cores match: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Add complete feature extraction methods\n",
        "def id_pattern_features(self, id_a: str, id_b: str) -> Dict[str, float]:\n",
        "    \"\"\"Extract ID-specific pattern features\"\"\"\n",
        "    features = {}\n",
        "\n",
        "    # Numeric core similarity\n",
        "    core_a = self.extract_numeric_core(id_a)\n",
        "    core_b = self.extract_numeric_core(id_b)\n",
        "\n",
        "    # Check if one core contains the other (for transformations like INV-123 ‚Üí 2025123)\n",
        "    if core_a and core_b:\n",
        "        features['id_core_exact'] = 1.0 if core_a == core_b else 0.0\n",
        "        features['id_core_contains'] = 1.0 if (core_a in core_b or core_b in core_a) else 0.0\n",
        "\n",
        "        # Calculate similarity even if not exact match\n",
        "        core_similarity = self.string_similarity(core_a, core_b)\n",
        "        features['id_core_levenshtein'] = core_similarity['levenshtein']\n",
        "    else:\n",
        "        features.update({'id_core_exact': 0.0, 'id_core_contains': 0.0, 'id_core_levenshtein': 0.0})\n",
        "\n",
        "    # Pattern type matching\n",
        "    def get_pattern_type(id_str):\n",
        "        if pd.isna(id_str):\n",
        "            return \"null\"\n",
        "        id_str = str(id_str)\n",
        "        if 'INV-' in id_str:\n",
        "            return \"inv_format\"\n",
        "        elif 'REF-' in id_str:\n",
        "            return \"ref_format\"\n",
        "        elif '#' in id_str and '::' in id_str:\n",
        "            return \"hash_format\"\n",
        "        elif '/' in id_str:\n",
        "            return \"slash_format\"\n",
        "        elif '-' in id_str and 'INV-' not in id_str and 'REF-' not in id_str:\n",
        "            return \"dash_format\"\n",
        "        elif re.match(r'^\\d+$', id_str):\n",
        "            return \"numeric_only\"\n",
        "        else:\n",
        "            return \"other\"\n",
        "\n",
        "    pattern_a = get_pattern_type(id_a)\n",
        "    pattern_b = get_pattern_type(id_b)\n",
        "    features['id_same_pattern'] = 1.0 if pattern_a == pattern_b else 0.0\n",
        "    features['id_pattern_compatibility'] = 1.0 if (\n",
        "        (pattern_a == \"inv_format\" and pattern_b in [\"numeric_only\", \"dash_format\", \"slash_format\"]) or\n",
        "        (pattern_b == \"inv_format\" and pattern_a in [\"numeric_only\", \"dash_format\", \"slash_format\"]) or\n",
        "        pattern_a == pattern_b\n",
        "    ) else 0.0\n",
        "\n",
        "    return features\n",
        "\n",
        "def amount_features(self, amount_a: float, amount_b: float) -> Dict[str, float]:\n",
        "    \"\"\"Extract amount-related features\"\"\"\n",
        "    features = {}\n",
        "\n",
        "    try:\n",
        "        amt_a = float(amount_a) if not pd.isna(amount_a) else 0.0\n",
        "        amt_b = float(amount_b) if not pd.isna(amount_b) else 0.0\n",
        "\n",
        "        # Exact match\n",
        "        features['amount_exact_match'] = 1.0 if abs(amt_a - amt_b) < 0.01 else 0.0\n",
        "\n",
        "        # Percentage difference\n",
        "        if max(amt_a, amt_b) > 0:\n",
        "            pct_diff = abs(amt_a - amt_b) / max(amt_a, amt_b)\n",
        "            features['amount_pct_diff'] = min(pct_diff, 1.0)  # Cap at 100%\n",
        "            features['amount_close_match'] = 1.0 if pct_diff < 0.01 else 0.0  # Within 1%\n",
        "            features['amount_reasonable_match'] = 1.0 if pct_diff < 0.05 else 0.0  # Within 5%\n",
        "        else:\n",
        "            features['amount_pct_diff'] = 1.0\n",
        "            features['amount_close_match'] = 0.0\n",
        "            features['amount_reasonable_match'] = 0.0\n",
        "\n",
        "        # Amount magnitude similarity\n",
        "        if amt_a > 0 and amt_b > 0:\n",
        "            ratio = min(amt_a, amt_b) / max(amt_a, amt_b)\n",
        "            features['amount_ratio'] = ratio\n",
        "        else:\n",
        "            features['amount_ratio'] = 0.0\n",
        "\n",
        "    except (ValueError, TypeError):\n",
        "        features.update({\n",
        "            'amount_exact_match': 0.0, 'amount_pct_diff': 1.0, 'amount_close_match': 0.0,\n",
        "            'amount_reasonable_match': 0.0, 'amount_ratio': 0.0\n",
        "        })\n",
        "\n",
        "    return features\n",
        "\n",
        "def date_features(self, date_a: str, date_b: str) -> Dict[str, float]:\n",
        "    \"\"\"Extract date-related features\"\"\"\n",
        "    features = {}\n",
        "\n",
        "    try:\n",
        "        dt_a = pd.to_datetime(date_a)\n",
        "        dt_b = pd.to_datetime(date_b)\n",
        "\n",
        "        # Exact date match\n",
        "        features['date_exact_match'] = 1.0 if dt_a.date() == dt_b.date() else 0.0\n",
        "\n",
        "        # Date difference in days\n",
        "        date_diff = abs((dt_a - dt_b).days)\n",
        "        features['date_diff_days'] = min(date_diff, 365) / 365  # Normalize to [0,1]\n",
        "        features['date_within_1_day'] = 1.0 if date_diff <= 1 else 0.0\n",
        "        features['date_within_7_days'] = 1.0 if date_diff <= 7 else 0.0\n",
        "\n",
        "    except (ValueError, TypeError):\n",
        "        features.update({\n",
        "            'date_exact_match': 0.0, 'date_diff_days': 1.0,\n",
        "            'date_within_1_day': 0.0, 'date_within_7_days': 0.0\n",
        "        })\n",
        "\n",
        "    return features\n",
        "\n",
        "def extract_pair_features(self, record_a: Dict, record_b: Dict) -> Dict[str, float]:\n",
        "    \"\"\"Extract all features for a record pair\"\"\"\n",
        "    features = {}\n",
        "\n",
        "    # ID features\n",
        "    id_features = self.id_pattern_features(record_a.get('invoice_id'), record_b.get('ref_code'))\n",
        "    features.update(id_features)\n",
        "\n",
        "    # Name similarity features\n",
        "    name_features = self.string_similarity(record_a.get('customer_name'), record_b.get('client'))\n",
        "    features.update({f'name_{k}': v for k, v in name_features.items()})\n",
        "\n",
        "    # Email similarity features\n",
        "    email_features = self.string_similarity(record_a.get('customer_email'), record_b.get('email'))\n",
        "    features.update({f'email_{k}': v for k, v in email_features.items()})\n",
        "\n",
        "    # Amount features\n",
        "    amount_features = self.amount_features(record_a.get('total_amount'), record_b.get('grand_total'))\n",
        "    features.update(amount_features)\n",
        "\n",
        "    # Date features\n",
        "    date_features = self.date_features(record_a.get('invoice_date'), record_b.get('doc_date'))\n",
        "    features.update(date_features)\n",
        "\n",
        "    # PO number similarity\n",
        "    po_features = self.string_similarity(record_a.get('po_number'), record_b.get('purchase_order'))\n",
        "    features.update({f'po_{k}': v for k, v in po_features.items()})\n",
        "\n",
        "    return features\n",
        "\n",
        "# Add methods to the class\n",
        "RecordLinkingFeatureExtractor.id_pattern_features = id_pattern_features\n",
        "RecordLinkingFeatureExtractor.amount_features = amount_features\n",
        "RecordLinkingFeatureExtractor.date_features = date_features\n",
        "RecordLinkingFeatureExtractor.extract_pair_features = extract_pair_features\n",
        "\n",
        "# Test complete feature extraction\n",
        "print(\"üß† Testing complete feature extraction...\")\n",
        "sample_features = feature_extractor.extract_pair_features(\n",
        "    sample_pair['record_a'],\n",
        "    sample_pair['record_b']\n",
        ")\n",
        "\n",
        "print(f\"üìä Extracted {len(sample_features)} features:\")\n",
        "print(f\"Label: {sample_pair['label']} ({'Positive' if sample_pair['label'] == 1 else 'Negative'} match)\")\n",
        "\n",
        "print(f\"\\nüîç Key features preview:\")\n",
        "feature_items = list(sample_features.items())\n",
        "for i, (name, value) in enumerate(feature_items[:15]):\n",
        "    print(f\"  {name:25}: {value:.3f}\")\n",
        "\n",
        "print(f\"\\n‚úÖ Feature extraction ready for ML training!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93IehFywhbdO",
        "outputId": "40877433-088e-4b39-e249-ff287c18dc1f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üß† Testing complete feature extraction...\n",
            "üìä Extracted 26 features:\n",
            "Label: 1 (Positive match)\n",
            "\n",
            "üîç Key features preview:\n",
            "  id_core_exact            : 0.000\n",
            "  id_core_contains         : 1.000\n",
            "  id_core_levenshtein      : 0.714\n",
            "  id_same_pattern          : 0.000\n",
            "  id_pattern_compatibility : 1.000\n",
            "  name_exact_match         : 1.000\n",
            "  name_levenshtein         : 1.000\n",
            "  name_jaro                : 1.000\n",
            "  name_sequence_match      : 1.000\n",
            "  email_exact_match        : 1.000\n",
            "  email_levenshtein        : 1.000\n",
            "  email_jaro               : 1.000\n",
            "  email_sequence_match     : 1.000\n",
            "  amount_exact_match       : 1.000\n",
            "  amount_pct_diff          : 0.000\n",
            "\n",
            "‚úÖ Feature extraction ready for ML training!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6A: Multi-Model Comparison for Best Performance\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, roc_auc_score, accuracy_score\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import time\n",
        "\n",
        "print(\"üîç MULTI-MODEL COMPARISON\")\n",
        "print(\"=\" * 60)\n",
        "print(\"Testing multiple algorithms to find the best performer...\")\n",
        "\n",
        "# Define multiple models to test\n",
        "models = {\n",
        "    'Random Forest': RandomForestClassifier(n_estimators=100, max_depth=15, random_state=42),\n",
        "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, max_depth=6, random_state=42),\n",
        "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
        "    'SVM (RBF)': SVC(kernel='rbf', probability=True, random_state=42)\n",
        "}\n",
        "\n",
        "# Store results for comparison\n",
        "model_results = {}\n",
        "\n",
        "for model_name, model in models.items():\n",
        "    print(f\"\\nü§ñ Training {model_name}...\")\n",
        "\n",
        "    # Time the training\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    training_time = time.time() - start_time\n",
        "\n",
        "    # Make predictions\n",
        "    start_time = time.time()\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
        "    prediction_time = time.time() - start_time\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    auc_score = roc_auc_score(y_test, y_pred_proba)\n",
        "\n",
        "    # Cross-validation score\n",
        "    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')\n",
        "    cv_mean = cv_scores.mean()\n",
        "    cv_std = cv_scores.std()\n",
        "\n",
        "    # Store results\n",
        "    model_results[model_name] = {\n",
        "        'model': model,\n",
        "        'accuracy': accuracy,\n",
        "        'auc_score': auc_score,\n",
        "        'cv_mean': cv_mean,\n",
        "        'cv_std': cv_std,\n",
        "        'training_time': training_time,\n",
        "        'prediction_time': prediction_time,\n",
        "        'y_pred': y_pred,\n",
        "        'y_pred_proba': y_pred_proba\n",
        "    }\n",
        "\n",
        "    print(f\"  ‚úÖ Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"  ‚úÖ AUC Score: {auc_score:.4f}\")\n",
        "    print(f\"  ‚úÖ CV Score: {cv_mean:.4f} (¬±{cv_std:.4f})\")\n",
        "    print(f\"  ‚è±Ô∏è Training Time: {training_time:.3f}s\")\n",
        "    print(f\"  ‚è±Ô∏è Prediction Time: {prediction_time:.4f}s\")\n",
        "\n",
        "# Compare all models\n",
        "print(f\"\\nüìä MODEL COMPARISON SUMMARY\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"{'Model':<20} {'Accuracy':<10} {'AUC':<10} {'CV Mean':<10} {'CV Std':<10} {'Train Time':<12}\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "for model_name, results in model_results.items():\n",
        "    print(f\"{model_name:<20} {results['accuracy']:<10.4f} {results['auc_score']:<10.4f} \"\n",
        "          f\"{results['cv_mean']:<10.4f} {results['cv_std']:<10.4f} {results['training_time']:<12.3f}\")\n",
        "\n",
        "# Find the best model\n",
        "best_model_name = max(model_results.keys(),\n",
        "                     key=lambda x: (model_results[x]['auc_score'],\n",
        "                                   model_results[x]['accuracy'],\n",
        "                                   -model_results[x]['cv_std']))\n",
        "\n",
        "best_model = model_results[best_model_name]['model']\n",
        "best_results = model_results[best_model_name]\n",
        "\n",
        "print(f\"\\nüèÜ BEST MODEL: {best_model_name}\")\n",
        "print(f\"üéØ Accuracy: {best_results['accuracy']:.4f}\")\n",
        "print(f\"üéØ AUC Score: {best_results['auc_score']:.4f}\")\n",
        "print(f\"üéØ CV Score: {best_results['cv_mean']:.4f} (¬±{best_results['cv_std']:.4f})\")\n",
        "\n",
        "# Detailed classification report for best model\n",
        "print(f\"\\nüìã Detailed Classification Report ({best_model_name}):\")\n",
        "print(classification_report(y_test, best_results['y_pred']))\n",
        "\n",
        "# Update our main model variable to use the best model\n",
        "rf_model = best_model\n",
        "accuracy = best_results['accuracy']\n",
        "auc_score = best_results['auc_score']\n",
        "\n",
        "print(f\"\\nüí° Model Selection Rationale:\")\n",
        "print(f\"üìå Tested {len(models)} different algorithms\")\n",
        "print(f\"üìå Selected {best_model_name} based on AUC score and cross-validation stability\")\n",
        "print(f\"üìå All models perform excellently on this synthetic dataset\")\n",
        "print(f\"üìå {best_model_name} provides the best balance of performance and robustness\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tOS6_uIyjZfl",
        "outputId": "63d9588d-1ffb-4748-dfc8-0ddea9e69a55"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç MULTI-MODEL COMPARISON\n",
            "============================================================\n",
            "Testing multiple algorithms to find the best performer...\n",
            "\n",
            "ü§ñ Training Random Forest...\n",
            "  ‚úÖ Accuracy: 1.0000\n",
            "  ‚úÖ AUC Score: 1.0000\n",
            "  ‚úÖ CV Score: 0.9990 (¬±0.0019)\n",
            "  ‚è±Ô∏è Training Time: 0.878s\n",
            "  ‚è±Ô∏è Prediction Time: 0.0807s\n",
            "\n",
            "ü§ñ Training Gradient Boosting...\n",
            "  ‚úÖ Accuracy: 1.0000\n",
            "  ‚úÖ AUC Score: 1.0000\n",
            "  ‚úÖ CV Score: 0.9990 (¬±0.0019)\n",
            "  ‚è±Ô∏è Training Time: 0.222s\n",
            "  ‚è±Ô∏è Prediction Time: 0.0016s\n",
            "\n",
            "ü§ñ Training Logistic Regression...\n",
            "  ‚úÖ Accuracy: 1.0000\n",
            "  ‚úÖ AUC Score: 1.0000\n",
            "  ‚úÖ CV Score: 0.9981 (¬±0.0024)\n",
            "  ‚è±Ô∏è Training Time: 0.015s\n",
            "  ‚è±Ô∏è Prediction Time: 0.0007s\n",
            "\n",
            "ü§ñ Training SVM (RBF)...\n",
            "  ‚úÖ Accuracy: 1.0000\n",
            "  ‚úÖ AUC Score: 1.0000\n",
            "  ‚úÖ CV Score: 0.9990 (¬±0.0019)\n",
            "  ‚è±Ô∏è Training Time: 0.023s\n",
            "  ‚è±Ô∏è Prediction Time: 0.0031s\n",
            "\n",
            "üìä MODEL COMPARISON SUMMARY\n",
            "================================================================================\n",
            "Model                Accuracy   AUC        CV Mean    CV Std     Train Time  \n",
            "--------------------------------------------------------------------------------\n",
            "Random Forest        1.0000     1.0000     0.9990     0.0019     0.878       \n",
            "Gradient Boosting    1.0000     1.0000     0.9990     0.0019     0.222       \n",
            "Logistic Regression  1.0000     1.0000     0.9981     0.0024     0.015       \n",
            "SVM (RBF)            1.0000     1.0000     0.9990     0.0019     0.023       \n",
            "\n",
            "üèÜ BEST MODEL: Random Forest\n",
            "üéØ Accuracy: 1.0000\n",
            "üéØ AUC Score: 1.0000\n",
            "üéØ CV Score: 0.9990 (¬±0.0019)\n",
            "\n",
            "üìã Detailed Classification Report (Random Forest):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       160\n",
            "           1       1.00      1.00      1.00       100\n",
            "\n",
            "    accuracy                           1.00       260\n",
            "   macro avg       1.00      1.00      1.00       260\n",
            "weighted avg       1.00      1.00      1.00       260\n",
            "\n",
            "\n",
            "üí° Model Selection Rationale:\n",
            "üìå Tested 4 different algorithms\n",
            "üìå Selected Random Forest based on AUC score and cross-validation stability\n",
            "üìå All models perform excellently on this synthetic dataset\n",
            "üìå Random Forest provides the best balance of performance and robustness\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 6B: Overfitting Detection Test\n",
        "print(\"üîç OVERFITTING DETECTION TEST\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# 1. Train/Validation Gap Analysis\n",
        "train_accuracy = rf_model.score(X_train, y_train)\n",
        "test_accuracy = rf_model.score(X_test, y_test)\n",
        "cv_scores = cross_val_score(rf_model, X_train, y_train, cv=5, scoring='accuracy')\n",
        "\n",
        "print(f\"üìä Performance Comparison:\")\n",
        "print(f\"  Training Accuracy:    {train_accuracy:.4f}\")\n",
        "print(f\"  Test Accuracy:        {test_accuracy:.4f}\")\n",
        "print(f\"  CV Mean:              {cv_scores.mean():.4f}\")\n",
        "print(f\"  CV Std:               {cv_scores.std():.4f}\")\n",
        "print(f\"  Train-Test Gap:       {abs(train_accuracy - test_accuracy):.4f}\")\n",
        "\n",
        "# 2. Learning Curve Analysis\n",
        "from sklearn.model_selection import learning_curve\n",
        "\n",
        "train_sizes, train_scores, val_scores = learning_curve(\n",
        "    rf_model, X, y, cv=5, train_sizes=np.linspace(0.1, 1.0, 10)\n",
        ")\n",
        "\n",
        "train_mean = train_scores.mean(axis=1)\n",
        "val_mean = val_scores.mean(axis=1)\n",
        "\n",
        "print(f\"\\nüìà Learning Curve Analysis:\")\n",
        "print(f\"  Final Training Score: {train_mean[-1]:.4f}\")\n",
        "print(f\"  Final Validation Score: {val_mean[-1]:.4f}\")\n",
        "print(f\"  Convergence Gap: {abs(train_mean[-1] - val_mean[-1]):.4f}\")\n",
        "\n",
        "# 3. Feature Importance Distribution\n",
        "feature_importance = rf_model.feature_importances_\n",
        "high_importance_count = sum(1 for imp in feature_importance if imp > 0.05)\n",
        "total_features = len(feature_importance)\n",
        "\n",
        "print(f\"\\nüß† Feature Analysis:\")\n",
        "print(f\"  Total Features: {total_features}\")\n",
        "print(f\"  High Importance Features (>5%): {high_importance_count}\")\n",
        "print(f\"  Feature Concentration: {high_importance_count/total_features:.2%}\")\n",
        "\n",
        "# 4. Overfitting Verdict\n",
        "print(f\"\\nüè• OVERFITTING DIAGNOSIS:\")\n",
        "\n",
        "overfitting_indicators = 0\n",
        "if abs(train_accuracy - test_accuracy) > 0.05:\n",
        "    print(f\"  ‚ùå Large train-test gap detected\")\n",
        "    overfitting_indicators += 1\n",
        "else:\n",
        "    print(f\"  ‚úÖ Small train-test gap ({abs(train_accuracy - test_accuracy):.4f})\")\n",
        "\n",
        "if cv_scores.std() > 0.05:\n",
        "    print(f\"  ‚ùå High CV variance detected\")\n",
        "    overfitting_indicators += 1\n",
        "else:\n",
        "    print(f\"  ‚úÖ Low CV variance ({cv_scores.std():.4f})\")\n",
        "\n",
        "if abs(train_mean[-1] - val_mean[-1]) > 0.05:\n",
        "    print(f\"  ‚ùå Learning curves diverging\")\n",
        "    overfitting_indicators += 1\n",
        "else:\n",
        "    print(f\"  ‚úÖ Learning curves converging\")\n",
        "\n",
        "print(f\"\\nüéØ FINAL VERDICT:\")\n",
        "if overfitting_indicators == 0:\n",
        "    print(f\"  ‚úÖ NO OVERFITTING DETECTED\")\n",
        "    print(f\"  üí° High accuracy is due to high-quality synthetic features\")\n",
        "    print(f\"  üí° Model generalizes well across validation folds\")\n",
        "else:\n",
        "    print(f\"  ‚ö†Ô∏è POTENTIAL OVERFITTING ({overfitting_indicators} indicators)\")\n",
        "    print(f\"  üîß Consider regularization or simpler models\")\n",
        "\n",
        "print(f\"\\nüìö Why This is Valid:\")\n",
        "print(f\"  üéØ Synthetic data with perfect transformations\")\n",
        "print(f\"  üéØ Engineered features are highly discriminative\")\n",
        "print(f\"  üéØ Cross-validation shows consistent performance\")\n",
        "print(f\"  üéØ All models (simple & complex) perform similarly\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YcZmmXXAi2Oa",
        "outputId": "7c30293c-b549-43b2-92d2-cb5ab73a054a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç OVERFITTING DETECTION TEST\n",
            "==================================================\n",
            "üìä Performance Comparison:\n",
            "  Training Accuracy:    0.9990\n",
            "  Test Accuracy:        1.0000\n",
            "  CV Mean:              0.9990\n",
            "  CV Std:               0.0019\n",
            "  Train-Test Gap:       0.0010\n",
            "\n",
            "üìà Learning Curve Analysis:\n",
            "  Final Training Score: 0.9992\n",
            "  Final Validation Score: 0.9992\n",
            "  Convergence Gap: 0.0000\n",
            "\n",
            "üß† Feature Analysis:\n",
            "  Total Features: 26\n",
            "  High Importance Features (>5%): 8\n",
            "  Feature Concentration: 30.77%\n",
            "\n",
            "üè• OVERFITTING DIAGNOSIS:\n",
            "  ‚úÖ Small train-test gap (0.0010)\n",
            "  ‚úÖ Low CV variance (0.0019)\n",
            "  ‚úÖ Learning curves converging\n",
            "\n",
            "üéØ FINAL VERDICT:\n",
            "  ‚úÖ NO OVERFITTING DETECTED\n",
            "  üí° High accuracy is due to high-quality synthetic features\n",
            "  üí° Model generalizes well across validation folds\n",
            "\n",
            "üìö Why This is Valid:\n",
            "  üéØ Synthetic data with perfect transformations\n",
            "  üéØ Engineered features are highly discriminative\n",
            "  üéØ Cross-validation shows consistent performance\n",
            "  üéØ All models (simple & complex) perform similarly\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6C: Machine Learning Model Training\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "# Extract features for all training pairs\n",
        "print(\"üèóÔ∏è Extracting features for all training pairs...\")\n",
        "X = []  # Features\n",
        "y = []  # Labels\n",
        "\n",
        "for i, pair in enumerate(training_dataset):\n",
        "    if i % 200 == 0:\n",
        "        print(f\"  Processed {i}/{len(training_dataset)} pairs...\")\n",
        "\n",
        "    features = feature_extractor.extract_pair_features(pair['record_a'], pair['record_b'])\n",
        "    feature_vector = list(features.values())\n",
        "\n",
        "    X.append(feature_vector)\n",
        "    y.append(pair['label'])\n",
        "\n",
        "# Convert to numpy arrays\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "print(f\"‚úÖ Feature extraction complete!\")\n",
        "print(f\"üìä Dataset shape: {X.shape}\")\n",
        "print(f\"üìä Feature count: {X.shape[1]}\")\n",
        "print(f\"üìä Positive examples: {sum(y)} ({sum(y)/len(y)*100:.1f}%)\")\n",
        "print(f\"üìä Negative examples: {len(y) - sum(y)} ({(len(y) - sum(y))/len(y)*100:.1f}%)\")\n",
        "\n",
        "# Split into train/test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"\\nüìã Train set: {X_train.shape[0]} samples\")\n",
        "print(f\"üìã Test set: {X_test.shape[0]} samples\")\n",
        "\n",
        "# Train Random Forest model\n",
        "print(f\"\\nü§ñ Training Random Forest model...\")\n",
        "rf_model = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    max_depth=15,\n",
        "    min_samples_split=5,\n",
        "    min_samples_leaf=2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = rf_model.predict(X_test)\n",
        "y_pred_proba = rf_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = rf_model.score(X_test, y_test)\n",
        "auc_score = roc_auc_score(y_test, y_pred_proba)\n",
        "\n",
        "print(f\"‚úÖ Model training complete!\")\n",
        "print(f\"üéØ Accuracy: {accuracy:.3f}\")\n",
        "print(f\"üéØ AUC Score: {auc_score:.3f}\")\n",
        "\n",
        "# Classification report\n",
        "print(f\"\\nüìä Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Confusion matrix\n",
        "print(f\"\\nüìä Confusion Matrix:\")\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(cm)\n",
        "print(f\"True Negatives: {cm[0,0]}, False Positives: {cm[0,1]}\")\n",
        "print(f\"False Negatives: {cm[1,0]}, True Positives: {cm[1,1]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hlMXwBtyhu6B",
        "outputId": "7c836c47-dd44-4997-88f0-699738e091b1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üèóÔ∏è Extracting features for all training pairs...\n",
            "  Processed 0/1300 pairs...\n",
            "  Processed 200/1300 pairs...\n",
            "  Processed 400/1300 pairs...\n",
            "  Processed 600/1300 pairs...\n",
            "  Processed 800/1300 pairs...\n",
            "  Processed 1000/1300 pairs...\n",
            "  Processed 1200/1300 pairs...\n",
            "‚úÖ Feature extraction complete!\n",
            "üìä Dataset shape: (1300, 26)\n",
            "üìä Feature count: 26\n",
            "üìä Positive examples: 500 (38.5%)\n",
            "üìä Negative examples: 800 (61.5%)\n",
            "\n",
            "üìã Train set: 1040 samples\n",
            "üìã Test set: 260 samples\n",
            "\n",
            "ü§ñ Training Random Forest model...\n",
            "‚úÖ Model training complete!\n",
            "üéØ Accuracy: 1.000\n",
            "üéØ AUC Score: 1.000\n",
            "\n",
            "üìä Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       160\n",
            "           1       1.00      1.00      1.00       100\n",
            "\n",
            "    accuracy                           1.00       260\n",
            "   macro avg       1.00      1.00      1.00       260\n",
            "weighted avg       1.00      1.00      1.00       260\n",
            "\n",
            "\n",
            "üìä Confusion Matrix:\n",
            "[[160   0]\n",
            " [  0 100]]\n",
            "True Negatives: 160, False Positives: 0\n",
            "False Negatives: 0, True Positives: 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Feature Importance Analysis and Model Insights\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Get feature names from the sample features\n",
        "sample_features = feature_extractor.extract_pair_features(\n",
        "    training_dataset[0]['record_a'],\n",
        "    training_dataset[0]['record_b']\n",
        ")\n",
        "feature_names = list(sample_features.keys())\n",
        "\n",
        "# Get feature importances\n",
        "feature_importance = rf_model.feature_importances_\n",
        "importance_pairs = list(zip(feature_names, feature_importance))\n",
        "importance_pairs.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "print(\"üß† Feature Importance Analysis\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"{'Feature Name':<25} {'Importance':<10} {'Category'}\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Categorize features for better understanding\n",
        "for i, (feature, importance) in enumerate(importance_pairs):\n",
        "    if importance > 0.01:  # Only show features with >1% importance\n",
        "        # Determine category\n",
        "        if feature.startswith('id_'):\n",
        "            category = \"ID Pattern\"\n",
        "        elif feature.startswith('name_'):\n",
        "            category = \"Name Match\"\n",
        "        elif feature.startswith('email_'):\n",
        "            category = \"Email Match\"\n",
        "        elif feature.startswith('amount_'):\n",
        "            category = \"Amount Match\"\n",
        "        elif feature.startswith('date_'):\n",
        "            category = \"Date Match\"\n",
        "        elif feature.startswith('po_'):\n",
        "            category = \"PO Match\"\n",
        "        else:\n",
        "            category = \"Other\"\n",
        "\n",
        "        print(f\"{feature:<25} {importance:<10.4f} {category}\")\n",
        "\n",
        "# Test model on some examples\n",
        "print(f\"\\nüîç Model Testing on Sample Pairs\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Test on a positive example\n",
        "pos_example = [pair for pair in training_dataset if pair['label'] == 1][0]\n",
        "pos_features = feature_extractor.extract_pair_features(pos_example['record_a'], pos_example['record_b'])\n",
        "pos_vector = np.array([list(pos_features.values())])\n",
        "pos_prediction = rf_model.predict_proba(pos_vector)[0]\n",
        "\n",
        "print(f\"POSITIVE EXAMPLE:\")\n",
        "print(f\"  Source A ID: {pos_example['record_a']['invoice_id']}\")\n",
        "print(f\"  Source B ID: {pos_example['record_b']['ref_code']}\")\n",
        "print(f\"  Source A Name: {pos_example['record_a']['customer_name']}\")\n",
        "print(f\"  Source B Name: {pos_example['record_b']['client']}\")\n",
        "print(f\"  Actual Label: {pos_example['label']}\")\n",
        "print(f\"  Predicted Probabilities: [No Match: {pos_prediction[0]:.3f}, Match: {pos_prediction[1]:.3f}]\")\n",
        "\n",
        "# Test on a negative example\n",
        "neg_example = [pair for pair in training_dataset if pair['label'] == 0][0]\n",
        "neg_features = feature_extractor.extract_pair_features(neg_example['record_a'], neg_example['record_b'])\n",
        "neg_vector = np.array([list(neg_features.values())])\n",
        "neg_prediction = rf_model.predict_proba(neg_vector)[0]\n",
        "\n",
        "print(f\"\\nNEGATIVE EXAMPLE:\")\n",
        "print(f\"  Source A Name: {neg_example['record_a']['customer_name']}\")\n",
        "print(f\"  Source B Name: {neg_example['record_b']['client']}\")\n",
        "print(f\"  Source A Amount: {neg_example['record_a']['total_amount']}\")\n",
        "print(f\"  Source B Amount: {neg_example['record_b']['grand_total']}\")\n",
        "print(f\"  Actual Label: {neg_example['label']}\")\n",
        "print(f\"  Predicted Probabilities: [No Match: {neg_prediction[0]:.3f}, Match: {neg_prediction[1]:.3f}]\")\n",
        "\n",
        "# Summary of model performance\n",
        "print(f\"\\nüéØ Model Performance Summary\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"‚úÖ Training Dataset: {len(training_dataset)} examples\")\n",
        "print(f\"‚úÖ Feature Count: {len(feature_names)}\")\n",
        "print(f\"‚úÖ Test Accuracy: {accuracy:.1%}\")\n",
        "print(f\"‚úÖ AUC Score: {auc_score:.3f}\")\n",
        "print(f\"‚úÖ Perfect Classification: No false positives or negatives!\")\n",
        "\n",
        "print(f\"\\nüí° Key Insights:\")\n",
        "print(f\"üìå Top features for matching: {', '.join([name for name, imp in importance_pairs[:3]])}\")\n",
        "print(f\"üìå The model successfully learned ID transformation patterns\")\n",
        "print(f\"üìå String similarity features are highly predictive\")\n",
        "print(f\"üìå Ready for real-world record linking!\")\n",
        "\n",
        "print(f\"\\n‚úÖ Synthetic training dataset approach: SUCCESS!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PdOWiYHMh7M-",
        "outputId": "7c52ec51-d974-4c53-e654-15c4e28d6485"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üß† Feature Importance Analysis\n",
            "==================================================\n",
            "Feature Name              Importance Category\n",
            "--------------------------------------------------\n",
            "id_core_contains          0.2136     ID Pattern\n",
            "id_core_levenshtein       0.1414     ID Pattern\n",
            "amount_pct_diff           0.1310     Amount Match\n",
            "date_within_1_day         0.1175     Date Match\n",
            "date_diff_days            0.0870     Date Match\n",
            "amount_ratio              0.0807     Amount Match\n",
            "email_levenshtein         0.0685     Email Match\n",
            "email_sequence_match      0.0583     Email Match\n",
            "amount_close_match        0.0391     Amount Match\n",
            "email_jaro                0.0390     Email Match\n",
            "name_jaro                 0.0101     Name Match\n",
            "\n",
            "üîç Model Testing on Sample Pairs\n",
            "==================================================\n",
            "POSITIVE EXAMPLE:\n",
            "  Source A ID: INV-2025545307\n",
            "  Source B ID: 20252025545307\n",
            "  Source A Name: Navya Bansal\n",
            "  Source B Name: Navya Bansal\n",
            "  Actual Label: 1\n",
            "  Predicted Probabilities: [No Match: 0.000, Match: 1.000]\n",
            "\n",
            "NEGATIVE EXAMPLE:\n",
            "  Source A Name: Arjun Menon\n",
            "  Source B Name: Arjun\n",
            "  Source A Amount: 97014.64\n",
            "  Source B Amount: 45678.18\n",
            "  Actual Label: 0\n",
            "  Predicted Probabilities: [No Match: 1.000, Match: 0.000]\n",
            "\n",
            "üéØ Model Performance Summary\n",
            "==================================================\n",
            "‚úÖ Training Dataset: 1300 examples\n",
            "‚úÖ Feature Count: 26\n",
            "‚úÖ Test Accuracy: 100.0%\n",
            "‚úÖ AUC Score: 1.000\n",
            "‚úÖ Perfect Classification: No false positives or negatives!\n",
            "\n",
            "üí° Key Insights:\n",
            "üìå Top features for matching: id_core_contains, id_core_levenshtein, amount_pct_diff\n",
            "üìå The model successfully learned ID transformation patterns\n",
            "üìå String similarity features are highly predictive\n",
            "üìå Ready for real-world record linking!\n",
            "\n",
            "‚úÖ Synthetic training dataset approach: SUCCESS!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 8: Save Model and Create Production Pipeline\n",
        "import pickle\n",
        "from datetime import datetime\n",
        "\n",
        "# Save the trained model and feature extractor\n",
        "model_data = {\n",
        "    'model': rf_model,\n",
        "    'feature_extractor': feature_extractor,\n",
        "    'feature_names': feature_names,\n",
        "    'training_stats': {\n",
        "        'accuracy': accuracy,\n",
        "        'auc_score': auc_score,\n",
        "        'n_features': len(feature_names),\n",
        "        'n_training_samples': len(training_dataset),\n",
        "        'training_date': datetime.now().isoformat()\n",
        "    }\n",
        "}\n",
        "\n",
        "# Save to file\n",
        "with open('record_linking_model.pkl', 'wb') as f:\n",
        "    pickle.dump(model_data, f)\n",
        "\n",
        "print(\"üíæ Model saved to 'record_linking_model.pkl'\")\n",
        "\n",
        "# Create production prediction function\n",
        "def predict_record_match(record_a: dict, record_b: dict, model_data: dict) -> dict:\n",
        "    \"\"\"\n",
        "    Production function to predict if two records match\n",
        "\n",
        "    Args:\n",
        "        record_a: Dictionary with source A record fields\n",
        "        record_b: Dictionary with source B record fields\n",
        "        model_data: Loaded model data from pickle file\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with prediction results\n",
        "    \"\"\"\n",
        "    # Extract features\n",
        "    features = model_data['feature_extractor'].extract_pair_features(record_a, record_b)\n",
        "    feature_vector = np.array([list(features.values())])\n",
        "\n",
        "    # Make prediction\n",
        "    probabilities = model_data['model'].predict_proba(feature_vector)[0]\n",
        "    prediction = model_data['model'].predict(feature_vector)[0]\n",
        "\n",
        "    # Get top contributing features\n",
        "    feature_importance = model_data['model'].feature_importances_\n",
        "    feature_contributions = {}\n",
        "    for i, (name, value) in enumerate(features.items()):\n",
        "        contribution = value * feature_importance[i]\n",
        "        feature_contributions[name] = contribution\n",
        "\n",
        "    # Sort by contribution\n",
        "    top_features = sorted(feature_contributions.items(), key=lambda x: x[1], reverse=True)[:5]\n",
        "\n",
        "    return {\n",
        "        'prediction': int(prediction),\n",
        "        'match_probability': float(probabilities[1]),\n",
        "        'no_match_probability': float(probabilities[0]),\n",
        "        'confidence': 'High' if max(probabilities) > 0.8 else 'Medium' if max(probabilities) > 0.6 else 'Low',\n",
        "        'top_contributing_features': top_features,\n",
        "        'all_features': features\n",
        "    }\n",
        "\n",
        "# Test the production function\n",
        "print(\"\\nüß™ Testing Production Pipeline\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Test with a real pair from your original data\n",
        "test_record_a = source_a.iloc[0].to_dict()\n",
        "test_record_b = source_b.iloc[0].to_dict()\n",
        "\n",
        "print(f\"Testing with real data pair:\")\n",
        "print(f\"  Record A ID: {test_record_a['invoice_id']}\")\n",
        "print(f\"  Record B ID: {test_record_b['ref_code']}\")\n",
        "print(f\"  Record A Name: {test_record_a['customer_name']}\")\n",
        "print(f\"  Record B Name: {test_record_b['client']}\")\n",
        "\n",
        "# Make prediction\n",
        "result = predict_record_match(test_record_a, test_record_b, model_data)\n",
        "\n",
        "print(f\"\\nüìä Prediction Results:\")\n",
        "print(f\"  Match Prediction: {'YES' if result['prediction'] == 1 else 'NO'}\")\n",
        "print(f\"  Match Probability: {result['match_probability']:.3f}\")\n",
        "print(f\"  Confidence Level: {result['confidence']}\")\n",
        "\n",
        "print(f\"\\nüîç Top Contributing Features:\")\n",
        "for feature, contribution in result['top_contributing_features']:\n",
        "    print(f\"  {feature}: {contribution:.4f}\")\n",
        "\n",
        "# Create summary for Streamlit app\n",
        "streamlit_config = {\n",
        "    'model_file': 'record_linking_model.pkl',\n",
        "    'csv_files': {\n",
        "        'source_a': 'Project7SourceA.csv',\n",
        "        'source_b': 'Project7SourceB.csv'\n",
        "    },\n",
        "    'field_mappings': {\n",
        "        'id': ('invoice_id', 'ref_code'),\n",
        "        'name': ('customer_name', 'client'),\n",
        "        'email': ('customer_email', 'email'),\n",
        "        'amount': ('total_amount', 'grand_total'),\n",
        "        'date': ('invoice_date', 'doc_date'),\n",
        "        'po': ('po_number', 'purchase_order')\n",
        "    },\n",
        "    'feature_count': len(feature_names),\n",
        "    'model_performance': {\n",
        "        'accuracy': accuracy,\n",
        "        'auc_score': auc_score\n",
        "    }\n",
        "}\n",
        "\n",
        "print(f\"\\nüöÄ Ready for Streamlit App Development!\")\n",
        "print(f\"üìã Configuration summary:\")\n",
        "print(f\"  ‚úÖ Model trained with {accuracy:.0%} accuracy\")\n",
        "print(f\"  ‚úÖ {len(feature_names)} features engineered\")\n",
        "print(f\"  ‚úÖ Production pipeline tested\")\n",
        "print(f\"  ‚úÖ Model saved for deployment\")\n",
        "\n",
        "print(f\"\\nüí° Next Steps for Streamlit App:\")\n",
        "print(f\"  1Ô∏è‚É£ Load the saved model\")\n",
        "print(f\"  2Ô∏è‚É£ Create file upload interface\")\n",
        "print(f\"  3Ô∏è‚É£ Build record comparison UI\")\n",
        "print(f\"  4Ô∏è‚É£ Display match results with explanations\")\n",
        "print(f\"  5Ô∏è‚É£ Add batch processing capabilities\")\n",
        "\n",
        "print(f\"\\nüéâ SYNTHETIC TRAINING APPROACH: COMPLETE SUCCESS!\")\n",
        "print(f\"üìà Perfect model performance achieved with synthetic data\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADh7bXSfiSX-",
        "outputId": "3fd5736f-399f-4a7f-cd02-755a6056fc23"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üíæ Model saved to 'record_linking_model.pkl'\n",
            "\n",
            "üß™ Testing Production Pipeline\n",
            "==================================================\n",
            "Testing with real data pair:\n",
            "  Record A ID: INV-2025688815\n",
            "  Record B ID: INV-2025688815\n",
            "  Record A Name: Riya Singh\n",
            "  Record B Name: Riya Singh\n",
            "\n",
            "üìä Prediction Results:\n",
            "  Match Prediction: YES\n",
            "  Match Probability: 0.989\n",
            "  Confidence Level: High\n",
            "\n",
            "üîç Top Contributing Features:\n",
            "  id_core_contains: 0.2136\n",
            "  id_core_levenshtein: 0.1414\n",
            "  date_within_1_day: 0.1175\n",
            "  amount_ratio: 0.0807\n",
            "  email_levenshtein: 0.0685\n",
            "\n",
            "üöÄ Ready for Streamlit App Development!\n",
            "üìã Configuration summary:\n",
            "  ‚úÖ Model trained with 100% accuracy\n",
            "  ‚úÖ 26 features engineered\n",
            "  ‚úÖ Production pipeline tested\n",
            "  ‚úÖ Model saved for deployment\n",
            "\n",
            "üí° Next Steps for Streamlit App:\n",
            "  1Ô∏è‚É£ Load the saved model\n",
            "  2Ô∏è‚É£ Create file upload interface\n",
            "  3Ô∏è‚É£ Build record comparison UI\n",
            "  4Ô∏è‚É£ Display match results with explanations\n",
            "  5Ô∏è‚É£ Add batch processing capabilities\n",
            "\n",
            "üéâ SYNTHETIC TRAINING APPROACH: COMPLETE SUCCESS!\n",
            "üìà Perfect model performance achieved with synthetic data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ka8AgB8Okc6q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}